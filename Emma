from tensorflow.keras.preprocessing.image import ImageDataGenerator
from keras.applications.vgg16 import VGG16
from keras.applications.vgg16 import preprocess_input
from keras.preprocessing.image import load_img
from keras.preprocessing.image import img_to_array
from keras import Sequential
from keras.layers import Flatten, Dense, Rescaling, Conv2D, MaxPooling2D # Import necessary layers
import matplotlib.pyplot as plt
import os
import csv
from keras.utils import to_categorical
import kagglehub
import shutil

from google.colab import drive
drive.mount("/content/drive")

# Download latest version
path = kagglehub.dataset_download("meowmeowmeowmeowmeow/gtsrb-german-traffic-sign")

print("Path to dataset files:", path)

# Instead of removing the directory, let's check if it exists first:
dir_to_remove = os.path.join('/root/.cache/kagglehub/datasets/meowmeowmeowmeowmeow/gtsrb-german-traffic-sign/versions/1/Train',str(11))
if os.path.exists(dir_to_remove):
    shutil.rmtree(dir_to_remove)
    print(f"Removed directory: {dir_to_remove}")
else:
    print(f"Directory not found: {dir_to_remove}")

train_ds = ImageDataGenerator(preprocessing_function=preprocess_input, validation_split=0.2) # Set validation_split here
train_data = '/root/.cache/kagglehub/datasets/meowmeowmeowmeowmeow/gtsrb-german-traffic-sign/versions/1/Train/'

# Create separate generators for training and validation
train = train_ds.flow_from_directory(train_data, target_size=(224, 224), color_mode='rgb', batch_size=32, class_mode='categorical', subset='training', shuffle=True)
validation = train_ds.flow_from_directory(train_data, target_size=(224, 224), color_mode='rgb', batch_size=32, class_mode='categorical', subset='validation', shuffle=False) # No need to shuffle validation data

# Get the actual number of classes from the generator
num_classes = train.num_classes
print(f"Number of classes found: {num_classes}")

# Define the VGG16 model as the base model
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the layers in the base model
for layer in base_model.layers:
    layer.trainable = False

# Create a new Sequential model
model = Sequential([
    base_model, # Add the VGG16 base model
    Flatten(),
    Dense(1024, activation='relu'),
    Dense(num_classes, activation='softmax')
])


#The Adam algorithm computes the exponential moving average of the gradient (first moment)
#and the squared gradient (second moment) of the weights, where the parameters beta1 and beta2
#control the smoothing rates of these moving averages
model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])

#model.summary()
#An epoch is when all the training data is used at once and is defined as the total number of iterations
#of all the training data in one cycle for training the machine learning model.
epochs = 7

# Use the validation generator in model.fit
history = model.fit(train, steps_per_epoch=20, epochs=epochs, validation_data=validation) # Use validation_data

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

plt.figure(figsize=(8, 8))
plt.subplot(2, 1, 1)
plt.plot(range(epochs), acc, label='Training Accuracy')
plt.plot(range(epochs), val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(range(epochs), loss, label='Training Loss')
plt.plot(range(epochs), val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

# Display the maximum validation accuracy
print("Maximum Validation Accuracy:", max(val_acc))
